{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Adaptable Experimentalist","text":"<ul> <li> <p>The Adaptable Experimentalist is a meta-sampling method that tries to optimize the use of existing sampling methods according to the current state of an optimization process. It does so by adapting the weights of the provided sampling methods, which are projected into an arbitrary sampler space that corresponds to a meta score function.</p> </li> <li> <p>We hope that such sampler spaces could yield appropriate sampling approaches for various stages of an optimization process, hopefully escaping the downsides of a one-size-fits-all approach. The idea could be further generalized in many aspects.</p> </li> <li> <p>The Adaptable Experimentalist expects a list of sampling methods, their corresponding projections into a sampler space, a list of models from previous optimization steps, an arbitrary meta score function, a temperature variable, besides usual parameters like the conditions pool and the number of desired samples.</p> <p>By default, the Adaptable Experimentalist uses a surprisal score function based on the Jensen-Shannon divergence between the current and previous models. This score is then used to adapt the weights of the provided sampling methods, assuming a one-dimensional sampler space where the lower-end samplers are better at lower surprisal scores, and vice versa.</p> </li> <li> <p>The Adaptable Experimentalist is a meta-sampling method. By default, it is set to use a set of novelty, falsification, model disagreement, and confirmation methods, as this fits well with the default meta score function.</p> </li> </ul> <p>We went with a meta method as we acknowledged that perhaps it is not the lack of sampling methods that is holding the sampling efficiency back, but rather a lack of adaptability to the current state of the optimization process. As we think sampling is highly tied to the naturally unknown proximity to the underlying truth, we aim to assess such proximity and adapt the sampling methods accordingly.</p>"},{"location":"Additional%20Example/","title":"Additional Example","text":"In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.autora_experimentalist_example import Example\n</pre> from autora.experimentalist.autora_experimentalist_example import Example <p>Include inline mathematics like this: $4 &lt; 5$</p> <p>Include block mathematics like this (don't forget the empty lines above and below the block):</p> <p>$$   y + 1 = 4  $$</p> <p>... or this:</p> <p>\\begin{align}     p(v_i=1|\\mathbf{h}) &amp; = \\sigma\\left(\\sum_j w_{ij}h_j + b_i\\right) \\\\     p(h_j=1|\\mathbf{v}) &amp; = \\sigma\\left(\\sum_i w_{ij}v_i + c_j\\right) \\end{align}</p>"},{"location":"Additional%20Example/#additional-example","title":"Additional Example\u00b6","text":""},{"location":"Basic%20Usage/","title":"Basic Usage","text":"<p>In the following, we guide you through how to use the adaptable experimentalist, specifically the adaptable_sample function. We provide some insights on internal workings as well.</p> <p>The adaptable experimentalist takes a list of sampling methods and an arbitrary projection of them into a meta-sampler space (currently a 1D line), and adapts the weights of the sampling methods based on a meta score function and a temperature parameter. By default, the meta score function used is a surprisal score based on the Jensen-Shannon divergence of previous models. This meta score is then mapped into the meta-sampler space, and the samplers are weighted with a Gaussian around that point. The temperature parameter is used to control the width of the Gaussian.</p> In\u00a0[1]: Copied! <pre>from autora.experimentalist.adaptable import adaptable_sample\nfrom autora.experimentalist.confirmation import confirmation_score_sample\nfrom autora.experimentalist.falsification import falsification_score_sample\nfrom autora.experimentalist.novelty import novelty_score_sample\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> from autora.experimentalist.adaptable import adaptable_sample from autora.experimentalist.confirmation import confirmation_score_sample from autora.experimentalist.falsification import falsification_score_sample from autora.experimentalist.novelty import novelty_score_sample  import numpy as np import matplotlib.pyplot as plt <p>Helper functions for giving some insight into the internal workings of the adaptable experimantalist:</p> In\u00a0[2]: Copied! <pre>import json\nimport os\n\n# reading some internal optionally saved values\ndef read_internal_values(index=0):\n    # read the inspection_plots\n    # get all the files in inspection_plots/samplers_space/ with the dir path included\n    samplers_spaces_files = os.listdir(\"inspection_plots/samplers_space/\")\n    surprisal_score_dists_files = os.listdir(\"inspection_plots/surprisal_score_dists/\")\n    # sort according to the date from newer to older\n    samplers_spaces_files.sort(key=lambda x: os.path.getmtime(\"inspection_plots/samplers_space/\" + x), reverse=True)\n    surprisal_score_dists_files.sort(key=lambda x: os.path.getmtime(\"inspection_plots/surprisal_score_dists/\" + x), reverse=True)\n\n    with open(\"inspection_plots/samplers_space/\" + samplers_spaces_files[index]) as f:\n        samplers_space = json.load(f)\n\n    with open(\"inspection_plots/surprisal_score_dists/\" + surprisal_score_dists_files[index]) as f:\n        surprisal_score_dists = json.load(f)\n    \n    return samplers_space, surprisal_score_dists\n\n# plot the surprisal score distributions and the samplers space\ndef plot_internal_values(samplers_space, surprisal_score_dists):\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n    ax[0].plot(samplers_space[\"x\"], samplers_space[\"y\"], label=\"gaussian distribution around the current progress\")\n    ax[0].axvline(x=samplers_space[\"mu\"], color=\"r\", linestyle=\"--\", label=\"current_meta_score\")\n    # ax[0]lt.axvline(x=samplers_coords[samplers_space[\"index_closest_sampler\"]], color=\"g\", linestyle=\"--\", label=\"closest sampler\")\n    ax[0].set_xticks(samplers_space[\"samplers_coords\"], samplers_space[\"samplers_names\"], rotation=45)\n    ax[0].legend()\n    ax[0].set_title(\n        f\"current tempreture: {np.round(samplers_space[\"temperature\"], 6)}, current meta score: {np.round(samplers_space['current_meta_score'], 6)} \\\n            \\nnormalized meta score projection: {np.round(samplers_space['current_projection'], 6)}\\\n            \\nmu:{np.round(samplers_space['mu'], 6)}, std:{samplers_space['std']}\"\n    )\n    ax[0].set_xlabel(\"samplers-space\")\n    ax[0].set_ylabel(\"density\")\n\n    ax[1].plot(surprisal_score_dists[\"shared_x\"], surprisal_score_dists[\"normalized_current_distribution\"], label=\"current\")\n    ax[1].plot(surprisal_score_dists[\"shared_x\"], surprisal_score_dists[\"normalized_prior_disribution\"], label=\"prior\")\n    # ax[1].set_suptitle(\"Normalized distributions\")\n    ax[1].set_title(f\"KL divergence: {np.round(surprisal_score_dists['score_kld'], 6)} | JS divergence: {np.round(surprisal_score_dists['score_jsd'], 6)}\")\n    ax[1].set_xlabel(\"observations\")\n    ax[1].set_ylabel(\"normalized density\")\n    ax[1].legend()\n\n    plt.tight_layout()\n    plt.show()\n</pre> import json import os  # reading some internal optionally saved values def read_internal_values(index=0):     # read the inspection_plots     # get all the files in inspection_plots/samplers_space/ with the dir path included     samplers_spaces_files = os.listdir(\"inspection_plots/samplers_space/\")     surprisal_score_dists_files = os.listdir(\"inspection_plots/surprisal_score_dists/\")     # sort according to the date from newer to older     samplers_spaces_files.sort(key=lambda x: os.path.getmtime(\"inspection_plots/samplers_space/\" + x), reverse=True)     surprisal_score_dists_files.sort(key=lambda x: os.path.getmtime(\"inspection_plots/surprisal_score_dists/\" + x), reverse=True)      with open(\"inspection_plots/samplers_space/\" + samplers_spaces_files[index]) as f:         samplers_space = json.load(f)      with open(\"inspection_plots/surprisal_score_dists/\" + surprisal_score_dists_files[index]) as f:         surprisal_score_dists = json.load(f)          return samplers_space, surprisal_score_dists  # plot the surprisal score distributions and the samplers space def plot_internal_values(samplers_space, surprisal_score_dists):     fig, ax = plt.subplots(1, 2, figsize=(12, 6))      ax[0].plot(samplers_space[\"x\"], samplers_space[\"y\"], label=\"gaussian distribution around the current progress\")     ax[0].axvline(x=samplers_space[\"mu\"], color=\"r\", linestyle=\"--\", label=\"current_meta_score\")     # ax[0]lt.axvline(x=samplers_coords[samplers_space[\"index_closest_sampler\"]], color=\"g\", linestyle=\"--\", label=\"closest sampler\")     ax[0].set_xticks(samplers_space[\"samplers_coords\"], samplers_space[\"samplers_names\"], rotation=45)     ax[0].legend()     ax[0].set_title(         f\"current tempreture: {np.round(samplers_space[\"temperature\"], 6)}, current meta score: {np.round(samplers_space['current_meta_score'], 6)} \\             \\nnormalized meta score projection: {np.round(samplers_space['current_projection'], 6)}\\             \\nmu:{np.round(samplers_space['mu'], 6)}, std:{samplers_space['std']}\"     )     ax[0].set_xlabel(\"samplers-space\")     ax[0].set_ylabel(\"density\")      ax[1].plot(surprisal_score_dists[\"shared_x\"], surprisal_score_dists[\"normalized_current_distribution\"], label=\"current\")     ax[1].plot(surprisal_score_dists[\"shared_x\"], surprisal_score_dists[\"normalized_prior_disribution\"], label=\"prior\")     # ax[1].set_suptitle(\"Normalized distributions\")     ax[1].set_title(f\"KL divergence: {np.round(surprisal_score_dists['score_kld'], 6)} | JS divergence: {np.round(surprisal_score_dists['score_jsd'], 6)}\")     ax[1].set_xlabel(\"observations\")     ax[1].set_ylabel(\"normalized density\")     ax[1].legend()      plt.tight_layout()     plt.show() <p>We demonstrate the usage of the adaptable_sample function on a simple example.</p> In\u00a0[3]: Copied! <pre># define a ground truth model for 1d experimental conditions\nground_truth = lambda x: np.sin(x) \n\nconditions_pool = np.linspace(0, 2*np.pi, 100)\n\nplt.plot(conditions_pool, ground_truth(conditions_pool), label='ground truth')\nplt.xlabel('conditions')\nplt.ylabel('response')\nplt.legend()\nplt.show()\n</pre> # define a ground truth model for 1d experimental conditions ground_truth = lambda x: np.sin(x)   conditions_pool = np.linspace(0, 2*np.pi, 100)  plt.plot(conditions_pool, ground_truth(conditions_pool), label='ground truth') plt.xlabel('conditions') plt.ylabel('response') plt.legend() plt.show() <p>Now we sample with the adaptable sampler with 3 sampling methods. Here, since we don't provide any meta score function, the default surprisal score is used and when no prior is provided a uniform prior is assumed. For illustration purposes, we will pass a prior identical to the current model, hence envoking one extreme case of the surprisal score, where the surprisal score is zero due to perfect match.</p> In\u00a0[4]: Copied! <pre>from sklearn.linear_model import LinearRegression\n\n# setting up the adaptable experimentalist\n\nreference_conditions = np.linspace(0.2*np.pi, 0.6*np.pi, 5)\nreference_observations = ground_truth(reference_conditions)\n\nmodel = LinearRegression()\nmodel.fit(reference_conditions.reshape(-1, 1), reference_observations)\nmodels = [model, model] # the models are used as references point to chnages in priors\n\n# defining the sampling methods\nsamplers = [\n    {\n            \"func\": novelty_score_sample,\n            \"name\": \"novelty\",\n            \"params\": {\"reference_conditions\": reference_conditions},\n        },\n        {\n            \"func\": falsification_score_sample,\n            \"name\": \"falsification\",\n            \"params\": {\n                \"reference_conditions\": reference_conditions,\n                \"reference_observations\": reference_observations,\n                # \"metadata\": meta_data,\n                \"model\": models[-1],\n            },\n        },\n        {\n            \"func\": confirmation_score_sample,\n            \"name\": \"confirmation\",\n            \"params\": {\n                \"reference_conditions\": reference_conditions,\n                \"reference_observations\": reference_observations,\n                # \"metadata\": meta_data,\n                \"model\": models[-1],\n            },\n        },\n    ]\n\nsamplers_coords = [1, 2, 4]\ntemperature = 0.1\nnum_samples = 10\n\nsampled_conditions = adaptable_sample(\n    conditions=conditions_pool.reshape(-1, 1),\n    reference_conditions=reference_conditions.reshape(-1, 1),\n    models=models,\n    samplers=samplers,\n    num_samples=num_samples,\n    samplers_coords=samplers_coords,\n    temperature=temperature,\n    plot_info=True,\n    )\n</pre> from sklearn.linear_model import LinearRegression  # setting up the adaptable experimentalist  reference_conditions = np.linspace(0.2*np.pi, 0.6*np.pi, 5) reference_observations = ground_truth(reference_conditions)  model = LinearRegression() model.fit(reference_conditions.reshape(-1, 1), reference_observations) models = [model, model] # the models are used as references point to chnages in priors  # defining the sampling methods samplers = [     {             \"func\": novelty_score_sample,             \"name\": \"novelty\",             \"params\": {\"reference_conditions\": reference_conditions},         },         {             \"func\": falsification_score_sample,             \"name\": \"falsification\",             \"params\": {                 \"reference_conditions\": reference_conditions,                 \"reference_observations\": reference_observations,                 # \"metadata\": meta_data,                 \"model\": models[-1],             },         },         {             \"func\": confirmation_score_sample,             \"name\": \"confirmation\",             \"params\": {                 \"reference_conditions\": reference_conditions,                 \"reference_observations\": reference_observations,                 # \"metadata\": meta_data,                 \"model\": models[-1],             },         },     ]  samplers_coords = [1, 2, 4] temperature = 0.1 num_samples = 10  sampled_conditions = adaptable_sample(     conditions=conditions_pool.reshape(-1, 1),     reference_conditions=reference_conditions.reshape(-1, 1),     models=models,     samplers=samplers,     num_samples=num_samples,     samplers_coords=samplers_coords,     temperature=temperature,     plot_info=True,     ) In\u00a0[5]: Copied! <pre># plot the sampled conditions\nplt.plot(conditions_pool, ground_truth(conditions_pool), label='ground truth')\nplt.scatter(sampled_conditions, ground_truth(sampled_conditions), label='sampled conditions', color='red', marker='x')\n# plot the reference conditions\nplt.scatter(reference_conditions, reference_observations, label='reference conditions', color='green')\nplt.xlabel('conditions')\nplt.ylabel('response')\nplt.legend()\nplt.show()\n</pre> # plot the sampled conditions plt.plot(conditions_pool, ground_truth(conditions_pool), label='ground truth') plt.scatter(sampled_conditions, ground_truth(sampled_conditions), label='sampled conditions', color='red', marker='x') # plot the reference conditions plt.scatter(reference_conditions, reference_observations, label='reference conditions', color='green') plt.xlabel('conditions') plt.ylabel('response') plt.legend() plt.show() In\u00a0[6]: Copied! <pre>samplers_space, surprisal_score_dists = read_internal_values()\nplot_internal_values(samplers_space, surprisal_score_dists)\nprint(\"samplers weights:\")\nprint(*zip(samplers_space[\"samplers_names\"], np.round(samplers_space[\"samplers_weights\"], 4)))\n</pre> samplers_space, surprisal_score_dists = read_internal_values() plot_internal_values(samplers_space, surprisal_score_dists) print(\"samplers weights:\") print(*zip(samplers_space[\"samplers_names\"], np.round(samplers_space[\"samplers_weights\"], 4)))  <pre>samplers weights:\n('novelty', 0.9961) ('falsification', 0.0039) ('confirmation', 0.0)\n</pre> <p>when the priors change, the experimantalist adapts based on its meta score function. Here, in case of high surprisal score, the model will tend to use more conformist sampling methods</p> In\u00a0[7]: Copied! <pre># we make a model that greatly diverges from the previous one\n# add too much noise to the fit data\nmodel = LinearRegression()\nmodel.fit(reference_conditions.reshape(-1, 1), reference_observations + np.random.normal(0, 0.5, reference_observations.shape))\nmodel.fit(reference_conditions.reshape(-1, 1), np.cos(reference_conditions))\n\nmodels.append(model)\n\ntemperature = 0.1\n\nsampled_conditions = adaptable_sample(\n    conditions=conditions_pool.reshape(-1, 1),\n    reference_conditions=reference_conditions.reshape(-1, 1),\n    models=models,\n    samplers=samplers,\n    num_samples=num_samples,\n    samplers_coords=samplers_coords,\n    temperature=temperature,\n    plot_info=True,\n    )\n</pre> # we make a model that greatly diverges from the previous one # add too much noise to the fit data model = LinearRegression() model.fit(reference_conditions.reshape(-1, 1), reference_observations + np.random.normal(0, 0.5, reference_observations.shape)) model.fit(reference_conditions.reshape(-1, 1), np.cos(reference_conditions))  models.append(model)  temperature = 0.1  sampled_conditions = adaptable_sample(     conditions=conditions_pool.reshape(-1, 1),     reference_conditions=reference_conditions.reshape(-1, 1),     models=models,     samplers=samplers,     num_samples=num_samples,     samplers_coords=samplers_coords,     temperature=temperature,     plot_info=True,     ) In\u00a0[8]: Copied! <pre># plot the sampled conditions\nplt.plot(conditions_pool, ground_truth(conditions_pool), label='ground truth')\nplt.scatter(sampled_conditions, ground_truth(sampled_conditions), label='sampled conditions', color='red', marker='x')\n# plot the reference conditions\nplt.scatter(reference_conditions, reference_observations, label='reference conditions', color='green')\nplt.xlabel('conditions')\nplt.ylabel('response')\nplt.legend()\nplt.show()\n</pre> # plot the sampled conditions plt.plot(conditions_pool, ground_truth(conditions_pool), label='ground truth') plt.scatter(sampled_conditions, ground_truth(sampled_conditions), label='sampled conditions', color='red', marker='x') # plot the reference conditions plt.scatter(reference_conditions, reference_observations, label='reference conditions', color='green') plt.xlabel('conditions') plt.ylabel('response') plt.legend() plt.show() In\u00a0[9]: Copied! <pre>samplers_space, surprisal_score_dists = read_internal_values()\nplot_internal_values(samplers_space, surprisal_score_dists)\nprint(\"samplers weights:\")\nprint(*zip(samplers_space[\"samplers_names\"], np.round(samplers_space[\"samplers_weights\"], 4)))\n</pre> samplers_space, surprisal_score_dists = read_internal_values() plot_internal_values(samplers_space, surprisal_score_dists) print(\"samplers weights:\") print(*zip(samplers_space[\"samplers_names\"], np.round(samplers_space[\"samplers_weights\"], 4)))  <pre>samplers weights:\n('novelty', 0.0) ('falsification', 0.0) ('confirmation', 1.0)\n</pre> <p>The effect of the tempreture is controling the width of the std of the gaussian, hence more exploratory weighting of the samplers.</p> In\u00a0[10]: Copied! <pre>temperature = 0.7\nsampled_conditions = adaptable_sample(\n    conditions=conditions_pool.reshape(-1, 1),\n    reference_conditions=reference_conditions.reshape(-1, 1),\n    models=models, \n    samplers=samplers,\n    num_samples=num_samples,\n    samplers_coords=samplers_coords,\n    temperature=temperature,\n    plot_info=True,\n    )\n</pre> temperature = 0.7 sampled_conditions = adaptable_sample(     conditions=conditions_pool.reshape(-1, 1),     reference_conditions=reference_conditions.reshape(-1, 1),     models=models,      samplers=samplers,     num_samples=num_samples,     samplers_coords=samplers_coords,     temperature=temperature,     plot_info=True,     ) In\u00a0[11]: Copied! <pre># plot the sampled conditions\nplt.plot(conditions_pool, ground_truth(conditions_pool), label='ground truth')\nplt.scatter(sampled_conditions, ground_truth(sampled_conditions), label='sampled conditions', color='red', marker='x')\n# plot the reference conditions\nplt.scatter(reference_conditions, reference_observations, label='reference conditions', color='green')\nplt.xlabel('conditions')\nplt.ylabel('response')\nplt.legend()\nplt.show()\n\nsamplers_space, surprisal_score_dists = read_internal_values()\nplot_internal_values(samplers_space, surprisal_score_dists)\nprint(\"samplers weights:\")\nprint(*zip(samplers_space[\"samplers_names\"], np.round(samplers_space[\"samplers_weights\"], 4)))\n</pre> # plot the sampled conditions plt.plot(conditions_pool, ground_truth(conditions_pool), label='ground truth') plt.scatter(sampled_conditions, ground_truth(sampled_conditions), label='sampled conditions', color='red', marker='x') # plot the reference conditions plt.scatter(reference_conditions, reference_observations, label='reference conditions', color='green') plt.xlabel('conditions') plt.ylabel('response') plt.legend() plt.show()  samplers_space, surprisal_score_dists = read_internal_values() plot_internal_values(samplers_space, surprisal_score_dists) print(\"samplers weights:\") print(*zip(samplers_space[\"samplers_names\"], np.round(samplers_space[\"samplers_weights\"], 4)))  <pre>samplers weights:\n('novelty', 0.1883) ('falsification', 0.3243) ('confirmation', 0.4874)\n</pre>"},{"location":"Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"Benchmark/","title":"Benchmar","text":"In\u00a0[1]: Copied! <pre># experimentalists\nfrom autora.experimentalist.adaptable import adaptable_sample\nfrom autora.experimentalist.confirmation import confirmation_score_sample\nfrom autora.experimentalist.falsification import falsification_score_sample\nfrom autora.experimentalist.model_disagreement import model_disagreement_score_sample\nfrom autora.experimentalist.novelty import novelty_score_sample\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.experimentalist.random import random_pool, random_sample\n\n# theorists\n# from autora.theorist.bms import BMSRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# experiment_runner\nfrom autora.experiment_runner.synthetic.psychology.luce_choice_ratio import luce_choice_ratio\nfrom autora.experiment_runner.synthetic.psychology.exp_learning import exp_learning\nfrom autora.experiment_runner.synthetic.psychology.q_learning import q_learning\nfrom autora.experiment_runner.synthetic.economics.expected_value_theory import expected_value_theory\nfrom autora.experiment_runner.synthetic.economics.prospect_theory import prospect_theory\nfrom autora.experiment_runner.synthetic.neuroscience.task_switching import task_switching\nfrom autora.experiment_runner.synthetic.psychophysics.stevens_power_law import stevens_power_law\nfrom autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law\n\n# autora state\nfrom autora.state import State, StandardState, on_state, estimator_on_state, Delta, VariableCollection\n\n# sklearn\nfrom sklearn.base import BaseEstimator\nfrom sklearn import linear_model\n\n# general\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List\nimport os\n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport logging\n\nlogging.basicConfig(level=logging.ERROR)\n</pre> # experimentalists from autora.experimentalist.adaptable import adaptable_sample from autora.experimentalist.confirmation import confirmation_score_sample from autora.experimentalist.falsification import falsification_score_sample from autora.experimentalist.model_disagreement import model_disagreement_score_sample from autora.experimentalist.novelty import novelty_score_sample from autora.experimentalist.grid import grid_pool from autora.experimentalist.random import random_pool, random_sample  # theorists # from autora.theorist.bms import BMSRegressor from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression  # experiment_runner from autora.experiment_runner.synthetic.psychology.luce_choice_ratio import luce_choice_ratio from autora.experiment_runner.synthetic.psychology.exp_learning import exp_learning from autora.experiment_runner.synthetic.psychology.q_learning import q_learning from autora.experiment_runner.synthetic.economics.expected_value_theory import expected_value_theory from autora.experiment_runner.synthetic.economics.prospect_theory import prospect_theory from autora.experiment_runner.synthetic.neuroscience.task_switching import task_switching from autora.experiment_runner.synthetic.psychophysics.stevens_power_law import stevens_power_law from autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law  # autora state from autora.state import State, StandardState, on_state, estimator_on_state, Delta, VariableCollection  # sklearn from sklearn.base import BaseEstimator from sklearn import linear_model  # general import numpy as np import pandas as pd import matplotlib.pyplot as plt from dataclasses import dataclass, field from typing import Optional, List import os  from concurrent.futures import ThreadPoolExecutor from tqdm import tqdm import logging  logging.basicConfig(level=logging.ERROR) <p>Utility classes and functions</p> In\u00a0[2]: Copied! <pre># Set up costum state\n# Here, we use a non-standard State to be able to use a multiple models\n@dataclass(frozen=True)\nclass CustomState(State):\n    variables: Optional[VariableCollection] = field(default=None, metadata={\"delta\": \"replace\"})\n    conditions: Optional[pd.DataFrame] = field(default=None, metadata={\"delta\": \"replace\", \"converter\": pd.DataFrame})\n    experiment_data: Optional[pd.DataFrame] = field(\n        default=None, metadata={\"delta\": \"extend\", \"converter\": pd.DataFrame}\n    )\n    models_lr: List[BaseEstimator] = field(\n        default_factory=list,\n        metadata={\"delta\": \"extend\"},\n    )\n    models_polyr: List[BaseEstimator] = field(\n        default_factory=list,\n        metadata={\"delta\": \"extend\"},\n    )\n\nclass PolynomialRegressor:\n    \"\"\"\n    This theorist fits a polynomial function to the data.\n    \"\"\"\n\n    def __init__(self, degree: int = 3):\n        self.poly = PolynomialFeatures(degree=degree, include_bias=False)\n        self.model = LinearRegression()\n\n    def fit(self, x, y):\n        features = self.poly.fit_transform(x, y)\n        self.model.fit(features, y)\n        return self\n\n    def predict(self, x):\n        features = self.poly.fit_transform(x)\n        return self.model.predict(features)\n\n    def print_eqn(self):\n        # Extract the coefficients and intercept\n        coeffs = self.model.coef_\n        intercept = self.model.intercept_\n\n        # Handle multi-output case by iterating over each output's coefficients and intercept\n        if coeffs.ndim &gt; 1:\n            for idx in range(coeffs.shape[0]):\n                equation = f\"y{idx+1} = {intercept[idx]:.3f}\"\n                feature_names = self.poly.get_feature_names_out()\n                for coef, feature in zip(coeffs[idx], feature_names):\n                    equation += f\" + ({coef:.3f}) * {feature}\"\n                print(equation)\n        else:\n            equation = f\"y = {intercept:.3f}\"\n            feature_names = self.poly.get_feature_names_out()\n            for coef, feature in zip(coeffs, feature_names):\n                equation += f\" + ({coef:.3f}) * {feature}\"\n            print(equation)\n\n\n# the following function is used to compute the model performance\n# on the validation set in terms of mean squared error\ndef get_validation_MSE(validation_experiment_data, working_state):\n    ivs = [iv.name for iv in validation_experiment_data.variables.independent_variables]\n    dvs = [dv.name for dv in validation_experiment_data.variables.dependent_variables]\n    X = validation_experiment_data.experiment_data[ivs]\n    y = validation_experiment_data.experiment_data[dvs]\n\n    y_pred_lr = working_state.models_lr[-1].predict(X)\n    y_pred_polyr = working_state.models_polyr[-1].predict(X)\n\n    MSE_lr = ((y - y_pred_lr) ** 2).mean().iloc[0]\n    MSE_polyr = ((y - y_pred_polyr) ** 2).mean().iloc[0]\n\n    min_MSE = min(MSE_lr, MSE_polyr)\n\n    return min_MSE\n\ndef plot_MSE(\n    benchmark_MSE_log,\n    working_MSE_log,\n    title=\"Single Discovery Simulation\",\n    filename=\"plots/single_discovery_simulation.png\",\n    save_plot=False,\n):\n    fig, ax = plt.subplots(1, 1)\n    ax.plot(benchmark_MSE_log, label=\"benchmark_MSE_log\")\n    ax.plot(working_MSE_log, label=\"working_MSE_log\")\n    ax.set_xlabel(\"Sampled Data Points\")\n    ax.set_ylabel(\"MSE on Validation Set\")\n    ax.set_title(title)\n    ax.legend()\n    plt.show()\n    if save_plot:\n        os.makedirs(\"plots\", exist_ok=True)\n        fig.savefig(filename)\n\ndef plot_mean_MSE_with_errorbar(\n    benchmark_MSE_plot_data,\n    working_MSE_plot_data,\n    num_cycles,\n    title=\"Averaged Discovery Simulations\",\n    filename=\"plots/averaged_discovery_simulations.png\",\n    save_plot=False,\n):\n    fig, ax = plt.subplots(1, 1)\n    ax.errorbar(\n        np.arange(num_cycles),\n        np.mean(benchmark_MSE_plot_data, axis=0),\n        yerr=np.std(benchmark_MSE_plot_data, axis=0),\n        label=\"benchmark_MSE_log\",\n    )\n    ax.errorbar(\n        np.arange(num_cycles),\n        np.mean(working_MSE_plot_data, axis=0),\n        yerr=np.std(working_MSE_plot_data, axis=0),\n        label=\"working_MSE_log\",\n    )\n    ax.set_xlabel(\"Sampled Data Points\")\n    ax.set_ylabel(\"MSE on Validation Setc\")\n    ax.set_title(title)\n    ax.legend()\n    plt.show()\n    if save_plot:\n        os.makedirs(\"plots\", exist_ok=True)\n        fig.savefig(filename)\n</pre> # Set up costum state # Here, we use a non-standard State to be able to use a multiple models @dataclass(frozen=True) class CustomState(State):     variables: Optional[VariableCollection] = field(default=None, metadata={\"delta\": \"replace\"})     conditions: Optional[pd.DataFrame] = field(default=None, metadata={\"delta\": \"replace\", \"converter\": pd.DataFrame})     experiment_data: Optional[pd.DataFrame] = field(         default=None, metadata={\"delta\": \"extend\", \"converter\": pd.DataFrame}     )     models_lr: List[BaseEstimator] = field(         default_factory=list,         metadata={\"delta\": \"extend\"},     )     models_polyr: List[BaseEstimator] = field(         default_factory=list,         metadata={\"delta\": \"extend\"},     )  class PolynomialRegressor:     \"\"\"     This theorist fits a polynomial function to the data.     \"\"\"      def __init__(self, degree: int = 3):         self.poly = PolynomialFeatures(degree=degree, include_bias=False)         self.model = LinearRegression()      def fit(self, x, y):         features = self.poly.fit_transform(x, y)         self.model.fit(features, y)         return self      def predict(self, x):         features = self.poly.fit_transform(x)         return self.model.predict(features)      def print_eqn(self):         # Extract the coefficients and intercept         coeffs = self.model.coef_         intercept = self.model.intercept_          # Handle multi-output case by iterating over each output's coefficients and intercept         if coeffs.ndim &gt; 1:             for idx in range(coeffs.shape[0]):                 equation = f\"y{idx+1} = {intercept[idx]:.3f}\"                 feature_names = self.poly.get_feature_names_out()                 for coef, feature in zip(coeffs[idx], feature_names):                     equation += f\" + ({coef:.3f}) * {feature}\"                 print(equation)         else:             equation = f\"y = {intercept:.3f}\"             feature_names = self.poly.get_feature_names_out()             for coef, feature in zip(coeffs, feature_names):                 equation += f\" + ({coef:.3f}) * {feature}\"             print(equation)   # the following function is used to compute the model performance # on the validation set in terms of mean squared error def get_validation_MSE(validation_experiment_data, working_state):     ivs = [iv.name for iv in validation_experiment_data.variables.independent_variables]     dvs = [dv.name for dv in validation_experiment_data.variables.dependent_variables]     X = validation_experiment_data.experiment_data[ivs]     y = validation_experiment_data.experiment_data[dvs]      y_pred_lr = working_state.models_lr[-1].predict(X)     y_pred_polyr = working_state.models_polyr[-1].predict(X)      MSE_lr = ((y - y_pred_lr) ** 2).mean().iloc[0]     MSE_polyr = ((y - y_pred_polyr) ** 2).mean().iloc[0]      min_MSE = min(MSE_lr, MSE_polyr)      return min_MSE  def plot_MSE(     benchmark_MSE_log,     working_MSE_log,     title=\"Single Discovery Simulation\",     filename=\"plots/single_discovery_simulation.png\",     save_plot=False, ):     fig, ax = plt.subplots(1, 1)     ax.plot(benchmark_MSE_log, label=\"benchmark_MSE_log\")     ax.plot(working_MSE_log, label=\"working_MSE_log\")     ax.set_xlabel(\"Sampled Data Points\")     ax.set_ylabel(\"MSE on Validation Set\")     ax.set_title(title)     ax.legend()     plt.show()     if save_plot:         os.makedirs(\"plots\", exist_ok=True)         fig.savefig(filename)  def plot_mean_MSE_with_errorbar(     benchmark_MSE_plot_data,     working_MSE_plot_data,     num_cycles,     title=\"Averaged Discovery Simulations\",     filename=\"plots/averaged_discovery_simulations.png\",     save_plot=False, ):     fig, ax = plt.subplots(1, 1)     ax.errorbar(         np.arange(num_cycles),         np.mean(benchmark_MSE_plot_data, axis=0),         yerr=np.std(benchmark_MSE_plot_data, axis=0),         label=\"benchmark_MSE_log\",     )     ax.errorbar(         np.arange(num_cycles),         np.mean(working_MSE_plot_data, axis=0),         yerr=np.std(working_MSE_plot_data, axis=0),         label=\"working_MSE_log\",     )     ax.set_xlabel(\"Sampled Data Points\")     ax.set_ylabel(\"MSE on Validation Setc\")     ax.set_title(title)     ax.legend()     plt.show()     if save_plot:         os.makedirs(\"plots\", exist_ok=True)         fig.savefig(filename) <p>On state functions</p> In\u00a0[3]: Copied! <pre># **** STATE WRAPPER FOR YOUR EXPERIMENTALIST ***\n@on_state()\ndef custom_sample_on_state(\n    experiment_data,\n    variables,\n    models_lr,\n    models_polyr,\n    all_conditions,\n    cycle=None,\n    max_cycle=None,\n    num_samples=1,\n    random_state=None,\n):\n\n    # very simple temperature regulation\n    # temperature = 0.6 * (1 - (cycle / max_cycle))\n    temperature = 0.1\n\n    # get the input relevant to some of the samplers\n    # independent and dependent variables for the metadata\n    iv = variables.independent_variables\n    dv = variables.dependent_variables\n    meta_data = VariableCollection(independent_variables=iv, dependent_variables=dv)\n\n    # reference conditions and observations\n    ivs = [iv.name for iv in variables.independent_variables]\n    dvs = [dv.name for dv in variables.dependent_variables]\n    reference_conditions = experiment_data[ivs]\n    reference_observations = experiment_data[dvs]\n\n    # remove the conditions that have already been sampled from the conditions pool\n    # remove reference conditions from the conditions pool\n    if isinstance(all_conditions, pd.DataFrame) and isinstance(reference_conditions, pd.DataFrame):\n        conditions_pool = pd.concat([all_conditions, reference_conditions])\n        conditions_pool = conditions_pool.drop_duplicates(keep=False)\n    else:\n        conditions_pool = all_conditions[~all_conditions.isin(reference_conditions)].dropna()\n\n    # NOTE: the sampler is performeing a bit worse when including falsification and confirmation\n    #       possiblly due to passing only one model to theses samplers\n    #       while the performance is based on 3 models\n    samplers = [\n        {\n            \"func\": novelty_score_sample,\n            \"name\": \"novelty\",\n            \"params\": {\"reference_conditions\": reference_conditions},\n        },\n        {\n            \"func\": falsification_score_sample,\n            \"name\": \"falsification\",\n            \"params\": {\n                \"reference_conditions\": reference_conditions,\n                \"reference_observations\": reference_observations,\n                \"metadata\": meta_data,\n                \"model\": models_polyr[-1],\n            },\n        },\n        {\n            \"func\": model_disagreement_score_sample,\n            \"name\": \"model_disagreement\",\n            \"params\": {\n                \"models\": [models_lr[-1], models_polyr[-1]],\n            },\n        },\n        {\n            \"func\": confirmation_score_sample,\n            \"name\": \"confirmation\",\n            \"params\": {\n                \"reference_conditions\": reference_conditions,\n                \"reference_observations\": reference_observations,\n                \"metadata\": meta_data,\n                \"model\": models_polyr[-1],\n            },\n        },\n    ]\n    # samplers_coords = [0, 1, 3, 4, 6]  # optional\n    # samplers_coords = [1, 2, 5]\n    samplers_coords = [0, 2, 3, 4]\n\n    adaptable_sampler_sensitivity = 12\n\n    new_conditions = adaptable_sample(\n        conditions=conditions_pool,\n        reference_conditions=reference_conditions,\n        models=models_polyr,  # pass only the polyr models\n        samplers=samplers,\n        num_samples=num_samples,\n        samplers_coords=samplers_coords,\n        sensitivity=adaptable_sampler_sensitivity,\n        temperature=temperature,\n        plot_info=False,\n    )\n\n    return Delta(conditions=new_conditions)\n\n# state wrapper for all theorists\n@on_state()\ndef theorists_on_state(experiment_data, variables):\n\n    # extract conditions X and observations y from experiment data\n    ivs = [iv.name for iv in variables.independent_variables]\n    dvs = [dv.name for dv in variables.dependent_variables]\n    X = experiment_data[ivs]\n    y = experiment_data[dvs]\n\n    # initialize and fit theorists\n    theorist_polyr = PolynomialRegressor()\n    theorist_lr = linear_model.LinearRegression()\n\n    return Delta(\n        models_lr=[theorist_lr.fit(X, y)], models_polyr=[theorist_polyr.fit(X, y)]\n    )\n\n# state wrapper for grid pooler experimentalist (generates a grid of experiment conditions)\n@on_state()\ndef grid_pool_on_state(variables):\n    return Delta(conditions=grid_pool(variables))\n\n# state wrapper for random pooler experimentalist (generates a pool of experiment conditions)\n@on_state()\ndef random_pool_on_state(variables, num_samples, random_state=None):\n    return Delta(conditions=random_pool(variables, num_samples, random_state))\n\n# state wrapper for random experimentalist (samples experiment conditions from a set of conditions)\n@on_state()\ndef random_sample_on_state(conditions, all_conditions, num_samples, random_state=None):\n    return Delta(conditions=random_sample(all_conditions, num_samples, random_state))\n\n# state wrapper for synthetic experiment runner\n@on_state()\ndef run_experiment_on_state(conditions, experiment_runner):\n    data = experiment_runner.run(conditions=conditions, added_noise=0.0)\n    return Delta(experiment_data=data)\n</pre> # **** STATE WRAPPER FOR YOUR EXPERIMENTALIST *** @on_state() def custom_sample_on_state(     experiment_data,     variables,     models_lr,     models_polyr,     all_conditions,     cycle=None,     max_cycle=None,     num_samples=1,     random_state=None, ):      # very simple temperature regulation     # temperature = 0.6 * (1 - (cycle / max_cycle))     temperature = 0.1      # get the input relevant to some of the samplers     # independent and dependent variables for the metadata     iv = variables.independent_variables     dv = variables.dependent_variables     meta_data = VariableCollection(independent_variables=iv, dependent_variables=dv)      # reference conditions and observations     ivs = [iv.name for iv in variables.independent_variables]     dvs = [dv.name for dv in variables.dependent_variables]     reference_conditions = experiment_data[ivs]     reference_observations = experiment_data[dvs]      # remove the conditions that have already been sampled from the conditions pool     # remove reference conditions from the conditions pool     if isinstance(all_conditions, pd.DataFrame) and isinstance(reference_conditions, pd.DataFrame):         conditions_pool = pd.concat([all_conditions, reference_conditions])         conditions_pool = conditions_pool.drop_duplicates(keep=False)     else:         conditions_pool = all_conditions[~all_conditions.isin(reference_conditions)].dropna()      # NOTE: the sampler is performeing a bit worse when including falsification and confirmation     #       possiblly due to passing only one model to theses samplers     #       while the performance is based on 3 models     samplers = [         {             \"func\": novelty_score_sample,             \"name\": \"novelty\",             \"params\": {\"reference_conditions\": reference_conditions},         },         {             \"func\": falsification_score_sample,             \"name\": \"falsification\",             \"params\": {                 \"reference_conditions\": reference_conditions,                 \"reference_observations\": reference_observations,                 \"metadata\": meta_data,                 \"model\": models_polyr[-1],             },         },         {             \"func\": model_disagreement_score_sample,             \"name\": \"model_disagreement\",             \"params\": {                 \"models\": [models_lr[-1], models_polyr[-1]],             },         },         {             \"func\": confirmation_score_sample,             \"name\": \"confirmation\",             \"params\": {                 \"reference_conditions\": reference_conditions,                 \"reference_observations\": reference_observations,                 \"metadata\": meta_data,                 \"model\": models_polyr[-1],             },         },     ]     # samplers_coords = [0, 1, 3, 4, 6]  # optional     # samplers_coords = [1, 2, 5]     samplers_coords = [0, 2, 3, 4]      adaptable_sampler_sensitivity = 12      new_conditions = adaptable_sample(         conditions=conditions_pool,         reference_conditions=reference_conditions,         models=models_polyr,  # pass only the polyr models         samplers=samplers,         num_samples=num_samples,         samplers_coords=samplers_coords,         sensitivity=adaptable_sampler_sensitivity,         temperature=temperature,         plot_info=False,     )      return Delta(conditions=new_conditions)  # state wrapper for all theorists @on_state() def theorists_on_state(experiment_data, variables):      # extract conditions X and observations y from experiment data     ivs = [iv.name for iv in variables.independent_variables]     dvs = [dv.name for dv in variables.dependent_variables]     X = experiment_data[ivs]     y = experiment_data[dvs]      # initialize and fit theorists     theorist_polyr = PolynomialRegressor()     theorist_lr = linear_model.LinearRegression()      return Delta(         models_lr=[theorist_lr.fit(X, y)], models_polyr=[theorist_polyr.fit(X, y)]     )  # state wrapper for grid pooler experimentalist (generates a grid of experiment conditions) @on_state() def grid_pool_on_state(variables):     return Delta(conditions=grid_pool(variables))  # state wrapper for random pooler experimentalist (generates a pool of experiment conditions) @on_state() def random_pool_on_state(variables, num_samples, random_state=None):     return Delta(conditions=random_pool(variables, num_samples, random_state))  # state wrapper for random experimentalist (samples experiment conditions from a set of conditions) @on_state() def random_sample_on_state(conditions, all_conditions, num_samples, random_state=None):     return Delta(conditions=random_sample(all_conditions, num_samples, random_state))  # state wrapper for synthetic experiment runner @on_state() def run_experiment_on_state(conditions, experiment_runner):     data = experiment_runner.run(conditions=conditions, added_noise=0.0)     return Delta(experiment_data=data)  <p>Automated close loop function</p> In\u00a0[4]: Copied! <pre>def run_simulation(num_cycles, num_conditions_per_cycle, num_initial_conditions, experiment_runner, sim=0):\n    # VALIDATION STATE\n    # at every step of our discovery process, we will evaluate the performance\n    # of the theorist against the ground truth. Here, we will define the ground\n    # truth as a grid of data points sampled across the domain of the experimental\n    # design space. We will store this validation set in a separate validation states\n\n    # create AutoRA state for validation purposes\n    # our validation set will be consist of a grid of experiment conditons\n    # across the entire experimental design domain\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        validation_conditions_future = executor.submit(\n            grid_pool_on_state, CustomState(variables=experiment_runner.variables)\n        )\n        validation_experiment_data_future = executor.submit(\n            grid_pool_on_state, CustomState(variables=experiment_runner.variables)\n        )\n\n    validation_conditions = validation_conditions_future.result()\n    validation_experiment_data = run_experiment_on_state(\n        validation_experiment_data_future.result(), experiment_runner=experiment_runner\n    )\n\n    benchmark_MSE_log = list()\n    working_MSE_log = list()\n\n    # INITIAL STATE\n    # We begin our discovery experiment with randomly sampled data set for 10\n    # conditions. We will use the same state for each experimentalist method.\n\n    # create initial AutoRA state which we will use for our discovery expeirments\n    initial_state = CustomState(variables=experiment_runner.variables)\n\n    # we will initiate our discovery process with 10 randomly sampled experiment conditions\n    initial_state = random_pool_on_state(initial_state, num_samples=num_initial_conditions, random_state=sim)\n\n    # we obtain the corresponding experiment data\n    initial_state = run_experiment_on_state(initial_state, experiment_runner=experiment_runner)\n\n    # initialize benchmark state for random experimentalist\n    benchmark_state = CustomState(**initial_state.__dict__)\n\n    # initialize working state for your custom experimentalist\n    working_state = CustomState(**initial_state.__dict__)\n\n    # for each discovery cycle\n    for cycle in tqdm(range(num_cycles), leave=True, desc=\"discovery cycles\"):\n\n        logging.info(\"SIMULATION \" + str(sim) + \" / DISCOVERY CYCLE \" + str(cycle))\n\n        # first, we fit a model to the data\n        with ThreadPoolExecutor(max_workers=2) as executor:\n            benchmark_future = executor.submit(theorists_on_state, benchmark_state)\n            working_future = executor.submit(theorists_on_state, working_state)\n\n        benchmark_state = benchmark_future.result()\n        working_state = working_future.result()\n\n        # now we can determine how well the models do on the validation set\n        # MSE calculation in parallel\n        with ThreadPoolExecutor(max_workers=2) as executor:\n            benchmark_MSE_future = executor.submit(get_validation_MSE, validation_experiment_data, benchmark_state)\n            working_MSE_future = executor.submit(get_validation_MSE, validation_experiment_data, working_state)\n\n        benchmark_MSE_log.append(benchmark_MSE_future.result())\n        working_MSE_log.append(working_MSE_future.result())\n\n        # then we determine the next experiment condition\n        # we obtain the corresponding experiment data\n        with ThreadPoolExecutor(max_workers=2) as executor:\n            benchmark_sample_future = executor.submit(\n                random_sample_on_state,\n                benchmark_state,\n                all_conditions=validation_conditions.conditions,\n                num_samples=num_conditions_per_cycle,\n            )\n            working_sample_future = executor.submit(\n                custom_sample_on_state,\n                working_state,\n                all_conditions=validation_conditions.conditions,\n                num_samples=num_conditions_per_cycle,\n                cycle=cycle,\n                max_cycle=num_cycles,\n            )\n\n        benchmark_state = benchmark_sample_future.result()\n        working_state = working_sample_future.result()\n\n        # we run the experiment and update the experiment data\n        with ThreadPoolExecutor(max_workers=2) as executor:\n            benchmark_experiment_future = executor.submit(\n                run_experiment_on_state, benchmark_state, experiment_runner=experiment_runner\n            )\n            working_experiment_future = executor.submit(\n                run_experiment_on_state, working_state, experiment_runner=experiment_runner\n            )\n\n        benchmark_state = benchmark_experiment_future.result()\n        working_state = working_experiment_future.result()\n\n    return benchmark_MSE_log, working_MSE_log, benchmark_state, working_state\n</pre> def run_simulation(num_cycles, num_conditions_per_cycle, num_initial_conditions, experiment_runner, sim=0):     # VALIDATION STATE     # at every step of our discovery process, we will evaluate the performance     # of the theorist against the ground truth. Here, we will define the ground     # truth as a grid of data points sampled across the domain of the experimental     # design space. We will store this validation set in a separate validation states      # create AutoRA state for validation purposes     # our validation set will be consist of a grid of experiment conditons     # across the entire experimental design domain     with ThreadPoolExecutor(max_workers=2) as executor:         validation_conditions_future = executor.submit(             grid_pool_on_state, CustomState(variables=experiment_runner.variables)         )         validation_experiment_data_future = executor.submit(             grid_pool_on_state, CustomState(variables=experiment_runner.variables)         )      validation_conditions = validation_conditions_future.result()     validation_experiment_data = run_experiment_on_state(         validation_experiment_data_future.result(), experiment_runner=experiment_runner     )      benchmark_MSE_log = list()     working_MSE_log = list()      # INITIAL STATE     # We begin our discovery experiment with randomly sampled data set for 10     # conditions. We will use the same state for each experimentalist method.      # create initial AutoRA state which we will use for our discovery expeirments     initial_state = CustomState(variables=experiment_runner.variables)      # we will initiate our discovery process with 10 randomly sampled experiment conditions     initial_state = random_pool_on_state(initial_state, num_samples=num_initial_conditions, random_state=sim)      # we obtain the corresponding experiment data     initial_state = run_experiment_on_state(initial_state, experiment_runner=experiment_runner)      # initialize benchmark state for random experimentalist     benchmark_state = CustomState(**initial_state.__dict__)      # initialize working state for your custom experimentalist     working_state = CustomState(**initial_state.__dict__)      # for each discovery cycle     for cycle in tqdm(range(num_cycles), leave=True, desc=\"discovery cycles\"):          logging.info(\"SIMULATION \" + str(sim) + \" / DISCOVERY CYCLE \" + str(cycle))          # first, we fit a model to the data         with ThreadPoolExecutor(max_workers=2) as executor:             benchmark_future = executor.submit(theorists_on_state, benchmark_state)             working_future = executor.submit(theorists_on_state, working_state)          benchmark_state = benchmark_future.result()         working_state = working_future.result()          # now we can determine how well the models do on the validation set         # MSE calculation in parallel         with ThreadPoolExecutor(max_workers=2) as executor:             benchmark_MSE_future = executor.submit(get_validation_MSE, validation_experiment_data, benchmark_state)             working_MSE_future = executor.submit(get_validation_MSE, validation_experiment_data, working_state)          benchmark_MSE_log.append(benchmark_MSE_future.result())         working_MSE_log.append(working_MSE_future.result())          # then we determine the next experiment condition         # we obtain the corresponding experiment data         with ThreadPoolExecutor(max_workers=2) as executor:             benchmark_sample_future = executor.submit(                 random_sample_on_state,                 benchmark_state,                 all_conditions=validation_conditions.conditions,                 num_samples=num_conditions_per_cycle,             )             working_sample_future = executor.submit(                 custom_sample_on_state,                 working_state,                 all_conditions=validation_conditions.conditions,                 num_samples=num_conditions_per_cycle,                 cycle=cycle,                 max_cycle=num_cycles,             )          benchmark_state = benchmark_sample_future.result()         working_state = working_sample_future.result()          # we run the experiment and update the experiment data         with ThreadPoolExecutor(max_workers=2) as executor:             benchmark_experiment_future = executor.submit(                 run_experiment_on_state, benchmark_state, experiment_runner=experiment_runner             )             working_experiment_future = executor.submit(                 run_experiment_on_state, working_state, experiment_runner=experiment_runner             )          benchmark_state = benchmark_experiment_future.result()         working_state = working_experiment_future.result()      return benchmark_MSE_log, working_MSE_log, benchmark_state, working_state <p>Next we put everything together and we run multiple cycles with both the adaptable experimentalist and random sampling on two experiments. Finally we compare the performance of the two methods.</p> In\u00a0[5]: Copied! <pre># meta parameters\nnum_cycles = 20\nnum_conditions_per_cycle = 1\nnum_initial_conditions = 1\n\n# YOU MAY CHANGE THESE PARAMETERS\nnum_discovery_simulations = 10\n\nexperiment_runners = [\n    # psychology\n    luce_choice_ratio(),\n    exp_learning(),\n    # economics\n    # expected_value_theory(),\n    prospect_theory(),\n    # neuroscience\n    # task_switching(),\n    # psychophysics\n    # weber_fechner_law(),\n]\n</pre> # meta parameters num_cycles = 20 num_conditions_per_cycle = 1 num_initial_conditions = 1  # YOU MAY CHANGE THESE PARAMETERS num_discovery_simulations = 10  experiment_runners = [     # psychology     luce_choice_ratio(),     exp_learning(),     # economics     # expected_value_theory(),     prospect_theory(),     # neuroscience     # task_switching(),     # psychophysics     # weber_fechner_law(), ] In\u00a0[\u00a0]: Copied! <pre># Single Discovery Simulations\nsingle_MSE_log = dict()\nfor experiment_runner in tqdm(\n    experiment_runners, total=len(experiment_runners), leave=True, desc=\"experiment runners single\"\n):\n    logging.info(\"## Running simulation for \" + experiment_runner.name)\n\n    benchmark_MSE_log, working_MSE_log, benchmark_state, working_state = run_simulation(\n        num_cycles, num_conditions_per_cycle, num_initial_conditions, experiment_runner\n    )\n    single_MSE_log[experiment_runner.name] = {\n        \"benchmark_MSE_log\": benchmark_MSE_log,\n        \"working_MSE_log\": working_MSE_log,\n    }\n\n    logging.info(\"## Finished simulation for \" + experiment_runner.name)\n    logging.info(\"## -----------------------------------------\")\n</pre> # Single Discovery Simulations single_MSE_log = dict() for experiment_runner in tqdm(     experiment_runners, total=len(experiment_runners), leave=True, desc=\"experiment runners single\" ):     logging.info(\"## Running simulation for \" + experiment_runner.name)      benchmark_MSE_log, working_MSE_log, benchmark_state, working_state = run_simulation(         num_cycles, num_conditions_per_cycle, num_initial_conditions, experiment_runner     )     single_MSE_log[experiment_runner.name] = {         \"benchmark_MSE_log\": benchmark_MSE_log,         \"working_MSE_log\": working_MSE_log,     }      logging.info(\"## Finished simulation for \" + experiment_runner.name)     logging.info(\"## -----------------------------------------\")  In\u00a0[7]: Copied! <pre># plot the results\nSAVE_PLOTS = False\nfor key, value in single_MSE_log.items():\n    plot_MSE(\n        value[\"benchmark_MSE_log\"],\n        value[\"working_MSE_log\"],\n        title=\"Single Discovery Simulation for \" + key,\n        filename=\"plots/single_sim_\" + key + \".png\",\n        save_plot=SAVE_PLOTS,\n    )\n</pre> # plot the results SAVE_PLOTS = False for key, value in single_MSE_log.items():     plot_MSE(         value[\"benchmark_MSE_log\"],         value[\"working_MSE_log\"],         title=\"Single Discovery Simulation for \" + key,         filename=\"plots/single_sim_\" + key + \".png\",         save_plot=SAVE_PLOTS,     ) In\u00a0[\u00a0]: Copied! <pre># Averaged Discovery Simulations over multiple runs\naverage_MSE_log = dict()\nfor experiment_runner in tqdm(\n    experiment_runners, total=len(experiment_runners), leave=True, desc=\"experiment runners average\"\n):\n    logging.info(\"## Running simulation for \" + experiment_runner.name)\n\n    benchmark_MSE_plot_data = np.zeros([num_discovery_simulations, num_cycles])\n    working_MSE_plot_data = np.zeros([num_discovery_simulations, num_cycles])\n\n    for sim in tqdm(\n        range(num_discovery_simulations), total=num_discovery_simulations, leave=True, desc=\"discovery simulations\"\n    ):\n        benchmark_MSE_log, working_MSE_log, benchmark_state, working_state = run_simulation(\n            num_cycles, num_conditions_per_cycle, num_initial_conditions, experiment_runner, sim\n        )\n\n        benchmark_MSE_plot_data[sim, :] = benchmark_MSE_log\n        working_MSE_plot_data[sim, :] = working_MSE_log\n\n    average_MSE_log[experiment_runner.name] = {\n        \"benchmark_MSE_log\": benchmark_MSE_plot_data,\n        \"working_MSE_log\": working_MSE_plot_data,\n    }\n\n    logging.info(\"## Finished simulation for \" + experiment_runner.name)\n    logging.info(\"## -----------------------------------------\")\n</pre>  # Averaged Discovery Simulations over multiple runs average_MSE_log = dict() for experiment_runner in tqdm(     experiment_runners, total=len(experiment_runners), leave=True, desc=\"experiment runners average\" ):     logging.info(\"## Running simulation for \" + experiment_runner.name)      benchmark_MSE_plot_data = np.zeros([num_discovery_simulations, num_cycles])     working_MSE_plot_data = np.zeros([num_discovery_simulations, num_cycles])      for sim in tqdm(         range(num_discovery_simulations), total=num_discovery_simulations, leave=True, desc=\"discovery simulations\"     ):         benchmark_MSE_log, working_MSE_log, benchmark_state, working_state = run_simulation(             num_cycles, num_conditions_per_cycle, num_initial_conditions, experiment_runner, sim         )          benchmark_MSE_plot_data[sim, :] = benchmark_MSE_log         working_MSE_plot_data[sim, :] = working_MSE_log      average_MSE_log[experiment_runner.name] = {         \"benchmark_MSE_log\": benchmark_MSE_plot_data,         \"working_MSE_log\": working_MSE_plot_data,     }      logging.info(\"## Finished simulation for \" + experiment_runner.name)     logging.info(\"## -----------------------------------------\") In\u00a0[9]: Copied! <pre># plot the results\nSAVE_PLOTS = False\nfor key, value in average_MSE_log.items():\n    plot_mean_MSE_with_errorbar(\n        value[\"benchmark_MSE_log\"],\n        value[\"working_MSE_log\"],\n        num_cycles,\n        title=\"Averaged Discovery Simulations for \" + key,\n        filename=\"plots/avg_sims_\" + key + \".png\",\n        save_plot=SAVE_PLOTS,\n    )\n</pre> # plot the results SAVE_PLOTS = False for key, value in average_MSE_log.items():     plot_mean_MSE_with_errorbar(         value[\"benchmark_MSE_log\"],         value[\"working_MSE_log\"],         num_cycles,         title=\"Averaged Discovery Simulations for \" + key,         filename=\"plots/avg_sims_\" + key + \".png\",         save_plot=SAVE_PLOTS,     )"},{"location":"Benchmark/#benchmarks-against-random-sampling","title":"Benchmarks against random sampling\u00b6","text":"<p>In the following we benchmark the performacnce of the adaptable experimantalist against randome sampling on two different experimanets in an automated close loop setting.</p>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>*adaptable-experimantalist is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experimentalist-adaptable\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.adaptable import Example\"\n</code></pre></p>"}]}